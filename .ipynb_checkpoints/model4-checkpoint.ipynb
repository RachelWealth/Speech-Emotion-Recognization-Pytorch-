{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e65572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有1440行\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-01-01-01-01-01.wav</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-01-01-01-02-01.wav</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-01-01-02-01-01.wav</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-01-01-02-02-01.wav</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-01-01-01-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-01-01-02-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-01-02-01-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-01-02-02-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-02-01-01-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>db/RAVDESS/Actor_01/03-01-02-02-01-02-01.wav</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           FILE EMOTION\n",
       "0  db/RAVDESS/Actor_01/03-01-01-01-01-01-01.wav      01\n",
       "1  db/RAVDESS/Actor_01/03-01-01-01-01-02-01.wav      01\n",
       "2  db/RAVDESS/Actor_01/03-01-01-01-02-01-01.wav      01\n",
       "3  db/RAVDESS/Actor_01/03-01-01-01-02-02-01.wav      01\n",
       "4  db/RAVDESS/Actor_01/03-01-02-01-01-01-01.wav      02\n",
       "5  db/RAVDESS/Actor_01/03-01-02-01-01-02-01.wav      02\n",
       "6  db/RAVDESS/Actor_01/03-01-02-01-02-01-01.wav      02\n",
       "7  db/RAVDESS/Actor_01/03-01-02-01-02-02-01.wav      02\n",
       "8  db/RAVDESS/Actor_01/03-01-02-02-01-01-01.wav      02\n",
       "9  db/RAVDESS/Actor_01/03-01-02-02-01-02-01.wav      02"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMOTIONS = {1:\"neu\",2:'cal',3:'hap',4:'sad',5:'ang',6:'fea',7:'dis',8:'sur'}\n",
    "EMOTION_NUM = 8\n",
    "TILTLEEMOTIONS={\"neu\":'中性','cal':'平静','hap':'快乐','sad':'悲伤','ang':'生气','fea':'害怕','dis':'厌恶','sur':'惊讶'}\n",
    "SOURCE_PATH = \"db/RAVDESS/\"\n",
    "SAMPLE_RATE = 48000\n",
    "EPOCH=10\n",
    "FILE_PATH=[]\n",
    "DURATION=3\n",
    "OFFSET=0.5\n",
    "\n",
    "df = pd.DataFrame(columns = ['FILE','EMOTION',])\n",
    "FILE_NAME=[]\n",
    "EMOTIONLIST=[]\n",
    "for director, _, file_names in os.walk(SOURCE_PATH):\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(director+'/',file_name)\n",
    "        FILE_NAME.append(file_path)\n",
    "        EMOTIONLIST.append(file_name.split('.')[0].split('-')[2])\n",
    "df[\"FILE\"]=FILE_NAME\n",
    "df[\"EMOTION\"]=EMOTIONLIST\n",
    "\n",
    "print(\"共有{}行\".format(df.shape[0]))\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef00b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index前10个:\t [135, 368, 750, 801, 1322, 1060, 613, 994, 527, 484]\n",
      "eval_index前10个:\t [1092, 495, 75, 1398, 215, 1108, 1279, 223, 735, 1053]\n",
      "test_index前十个:\t [65, 1093, 925, 1170, 536, 675, 1417, 921, 1081, 1383]\n",
      "train_index/(train_index+test_eval_index)=1152/(1152+288)=0.8\n",
      "eval_index/(test_index+eval_index)=144/(144+144)=0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_index,test_eval_index=train_test_split(list(df.index),test_size=0.2,random_state=1)\n",
    "test_index,eval_index=train_test_split(test_eval_index,test_size=0.5,random_state=1)\n",
    "\n",
    "print(\"train_index前10个:\\t\",train_index[:10])\n",
    "print(\"eval_index前10个:\\t\",eval_index[:10])\n",
    "print(\"test_index前十个:\\t\",test_index[:10])\n",
    "print('train_index/(train_index+test_eval_index)={}/({}+{})={}'.format(len(train_index),len(train_index),len(test_eval_index),len(train_index)/(len(train_index)+len(test_eval_index))))\n",
    "print('eval_index/(test_index+eval_index)={}/({}+{})={}'.format(len(eval_index),len(test_index),len(eval_index),len(eval_index)/(len(eval_index)+len(test_index))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92bab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def getMELspec(data,sr):\n",
    "    # shape=(n_mels, t)\n",
    "    mel_spec = librosa.feature.melspectrogram(y = data,\n",
    "                                              sr = SAMPLE_RATE,\n",
    "                                              n_fft=1024,      # length of the FFT window\n",
    "                                              win_length=1024,\n",
    "                                              window='hamming',\n",
    "                                              hop_length=512,\n",
    "                                              n_mels=64\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec,ref=np.max)\n",
    "    mfccs = librosa.feature.mfcc(S=mel_spec_db)\n",
    "    return mel_spec_db,mfccs\n",
    "\n",
    "y_data=np.array(df['EMOTION'])\n",
    "y_data=y_data.astype(np.int32)\n",
    "y_data=y_data-1\n",
    "\n",
    "alpha = 0.97\n",
    "def PreEmphsised(data):\n",
    "    return np.append(data[0],data[1:] - alpha * data[:-1])\n",
    "\n",
    "def sameLenData(data):\n",
    "    smLenData = np.zeros(SAMPLE_RATE*DURATION)\n",
    "    smLenData[0:len(data)]=data\n",
    "    return smLenData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f8bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "scaler = StandardScaler()\n",
    "DURATION=3\n",
    "OFFSET=0.5\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self,index,setName=None):\n",
    "        self.x_data = []\n",
    "        self.arg = np.zeros([1,144000])\n",
    "        self.y_data = []\n",
    "        with tqdm(total=len(index)+1,desc=\"数据加载\") as loadbar:\n",
    "            for i in index: # 写入1440个文件的数据\n",
    "                data, sr = librosa.load(df['FILE'][i],sr=SAMPLE_RATE,duration=DURATION,offset=OFFSET)\n",
    "                smLenData = sameLenData(data)\n",
    "                # 预加重\n",
    "                smLenData = PreEmphsised(smLenData)\n",
    "                # 特征向量\n",
    "                mel_data,mfcc_data = getMELspec(smLenData,sr)\n",
    "                # 差分向量\n",
    "                #_,_,mel_data = get3DMel(mel_data)\n",
    "                self.x_data.append(list(mel_data))\n",
    "                self.y_data.append(y_data[i])\n",
    "                loadbar.update(1)\n",
    "#             if setName=='train_index':\n",
    "#                 self.x_data=np.concatenate([self.x_data,arg_data],axis=0)\n",
    "#                 self.y_data=np.concatenate([self.y_data,arg_label],axis=0)\n",
    "            self.x_data=np.expand_dims(self.x_data,1)\n",
    "            shape=np.array(self.x_data).shape\n",
    "            self.x_data = np.reshape(self.x_data,newshape=(shape[0],-1))\n",
    "            if setName=='train_index':\n",
    "                self.x_data = scaler.fit_transform(self.x_data)\n",
    "            else:\n",
    "                self.x_data = scaler.transform(self.x_data)\n",
    "            self.x_data=np.reshape(self.x_data,newshape=shape)\n",
    "            self.x_data=torch.from_numpy(np.array(self.x_data,dtype=np.float64))\n",
    "            self.y_data=torch.from_numpy(np.array(self.y_data,dtype=np.float64))\n",
    "  \n",
    "            self.len = len(index)\n",
    "            loadbar.update(1)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1d906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "数据加载: 100%|████████████████████████████████████████████████████████████████████| 1153/1153 [00:48<00:00, 23.96it/s]\n",
      "数据加载: 100%|██████████████████████████████████████████████████████████████████████| 145/145 [00:04<00:00, 29.59it/s]\n",
      "数据加载: 100%|██████████████████████████████████████████████████████████████████████| 145/145 [00:05<00:00, 28.03it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "# 训练集加载\n",
    "train_dataset = SpeechDataset(train_index,'train_index')\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                         )\n",
    "# 验证集加载\n",
    "eval_dataset = SpeechDataset(eval_index)\n",
    "eval_loader = DataLoader(eval_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                         )\n",
    "# 测试集加载\n",
    "test_dataset = SpeechDataset(test_index)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed9045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1152, 1, 64, 282])\n",
      "torch.Size([144, 1, 64, 282])\n",
      "torch.Size([144, 1, 64, 282])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x_data.shape)\n",
    "print(eval_dataset.x_data.shape)\n",
    "print(test_dataset.x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178b602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: torch.Size([1152, 1, 64, 282])   y_data torch.Size([1152])\n",
      "[[[-0.67325471 -0.6821769  -0.69511429 ... -0.63889715 -0.64009395\n",
      "   -0.12269867]\n",
      "  [-0.62930198 -0.59434941 -0.60018943 ... -0.55454918 -0.56611444\n",
      "    0.14377254]\n",
      "  [-0.61070813 -0.5604981  -0.57121697 ... -0.5390266  -0.5435462\n",
      "   -0.06885969]\n",
      "  ...\n",
      "  [-1.04204697 -1.06620386 -1.07144973 ...  0.01656576 -0.19195955\n",
      "   -0.21951804]\n",
      "  [-1.0418595  -1.07270438 -1.07460435 ... -0.04612548 -0.30882887\n",
      "   -0.15334563]\n",
      "  [-1.03902234 -1.06338046 -1.07141052 ... -0.09199534 -0.34217952\n",
      "   -0.38634281]]]\n",
      "[[[2.15082589 0.50382796 0.43699593 ... 3.69490294 3.83674473 3.89645083]\n",
      "  [2.38720546 1.01889965 1.01030396 ... 1.4825624  1.45573554 1.25276435]\n",
      "  [1.89543636 1.08976406 1.02939147 ... 0.51136153 0.52635476 0.48685327]\n",
      "  ...\n",
      "  [2.03571829 1.45808851 1.78926593 ... 2.24626856 2.29379796 2.1371975 ]\n",
      "  [2.08007865 1.46623807 1.63826211 ... 2.16340767 2.16687631 2.19449819]\n",
      "  [2.16775224 1.86209643 1.72413553 ... 2.14209925 2.45407809 2.25984235]]]\n",
      "[[[-0.4257037  -0.43329915 -0.44958875 ... -0.43081116 -0.42947406\n",
      "   -0.43211949]\n",
      "  [-0.31297175 -0.24447725 -0.2509149  ... -0.31365687 -0.32312423\n",
      "   -0.3400689 ]\n",
      "  [-0.29416954 -0.20259875 -0.22408623 ...  0.04502089 -0.30298551\n",
      "   -0.31527431]\n",
      "  ...\n",
      "  [-0.82520401 -0.84708481 -0.85089225 ...  1.00757977  0.52584528\n",
      "    0.54357526]\n",
      "  [-0.82529567 -0.85293428 -0.85382905 ...  0.97437292  0.77931921\n",
      "    0.69027301]\n",
      "  [-0.81939792 -0.83854671 -0.84617996 ...  1.24546998  1.03420691\n",
      "    0.7570671 ]]]\n"
     ]
    }
   ],
   "source": [
    "print('x_data:',train_dataset.x_data.shape,'  y_data',train_dataset.y_data.shape)\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "print(np.array(train_dataset.x_data[0]))\n",
    "print(np.array(eval_dataset.x_data[0]))\n",
    "print(np.array(test_dataset.x_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144dacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Inception, self).__init__()\n",
    "        self.block1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.block5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.block5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "        self.block3x3_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.block3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.block3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "        self.block_pool = torch.nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        block1x1 = self.block1x1(x)\n",
    "        block5x5 = self.block5x5_2(self.block5x5_1(x))\n",
    "        block3x3 = self.block3x3_3(self.block3x3_2(self.block3x3_1(x)))\n",
    "        block_pool = self.block_pool(F.avg_pool2d(x, kernel_size=3, stride=1, padding=1))\n",
    "        outputs = [block1x1, block3x3, block5x5, block_pool]\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04672182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class attention(nn.Module):\n",
    "    def __init__(self,en_hidden_dim,de_hidden_dim,direction):\n",
    "        super().__init__()\n",
    "        middle_size = de_hidden_dim\n",
    "        # H与s0拼接出来为(batch_size,src_len,en_hidden_dim*direction+de_hidden_dim)\n",
    "        # 需要转化维度。首先，a(batch_size,src_len),公式2得知v(batch_size,?),E(?,src_len)\n",
    "        # 因此?可以是任意维度，此处取de_hidden_dim\n",
    "        self.s_en2de=nn.Linear(en_hidden_dim*2,de_hidden_dim)\n",
    "        self.attn=nn.Linear(en_hidden_dim*direction+de_hidden_dim,middle_size)\n",
    "        self.v = nn.Linear(middle_size,1)\n",
    "        \n",
    "    def forward(self,gru_output,gru_hidden):       \n",
    "        # H(batch_size,src_len,en_hidden_dim*direction)\n",
    "        _,src_len,_=gru_output.shape\n",
    "        # s0(batch_size,en_hidden_dim*direction)\n",
    "        # 但是s0作为dcoder初始隐藏状态，应该是s0(batch_size,de_hidden_dim)\n",
    "        # 并且为了与H拼接，需要加一个维度，变成s0(batch_size,src_len,de_hidden_dim)\n",
    "        s = torch.cat((gru_hidden[:, -2,: ], gru_hidden[:, -1,:]),dim=1)  # (batch_size,en_hidden_dim*direction)\n",
    "        s = self.s_en2de(s)  # (batch_size,de_hidden_dim)\n",
    "        # s=torc.tanh(s)  \n",
    "        s=s.unsqueeze(1).repeat(1,src_len,1) # (batch_size,src_len,de_hidden_dim)     \n",
    "        self.attn_hidden = torch.tanh(self.attn(torch.cat((s,gru_output),dim=2)))     \n",
    "        attention_weight = self.v(self.attn_hidden).squeeze(2) #(batch_size,1,src_len)\n",
    "        \n",
    "        return F.softmax(attention_weight,dim=1)   #A dimension along which Softmax will be computed (so every slice along dim will sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1399f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size,input_size,p=128,16,0.3\n",
    "kernel_size,stride=4,4\n",
    "directional = 2\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "         input dimension every epoch is [32,1,128,282]\n",
    "        \"\"\"\n",
    "        # conv2d\n",
    "#         self.conv2d=nn.Sequential(# No 1\n",
    "#                                    nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1,padding=1)\n",
    "#                                   ,nn.BatchNorm2d(8)\n",
    "#                                   ,nn.ReLU()\n",
    "#                                   ,nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "#                                   ,nn.Dropout(p)\n",
    "                                    \n",
    "#                                   #,Inception(8)  #outchannel 88     35840\n",
    "                                  \n",
    "#                                   ,ResidualBlock(8)\n",
    "            \n",
    "#                                   ,nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "#                                   ,nn.BatchNorm2d(128)\n",
    "#                                   ,nn.ReLU()\n",
    "#                                   ,nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "#                                   ,nn.Dropout(p)\n",
    "#         )\n",
    "        self.conv2d=nn.Sequential(# No 1\n",
    "                                   nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "                                  ,nn.BatchNorm2d(16)\n",
    "                                  ,nn.ReLU()\n",
    "                                  ,nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "                                  ,nn.Dropout(p)\n",
    "            \n",
    "                                  # No 2\n",
    "                                  ,nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "                                  ,nn.BatchNorm2d(32)\n",
    "                                  ,nn.ReLU()\n",
    "                                  ,nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "                                  ,nn.Dropout(p)\n",
    "            \n",
    "                                  # No 3\n",
    "                                  ,nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "                                  ,nn.BatchNorm2d(64)\n",
    "                                  ,nn.ReLU()\n",
    "                                  ,nn.MaxPool2d(kernel_size=4,stride=4)\n",
    "                                  ,nn.Dropout(p)\n",
    "            \n",
    "                                  # No 4\n",
    "                                  ,nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "                                  ,nn.BatchNorm2d(64)\n",
    "                                  ,nn.ReLU()\n",
    "                                  ,nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "                                  ,nn.Dropout(p)\n",
    "        )\n",
    "        \n",
    "        # GRU\n",
    "        self.mp = nn.MaxPool2d(kernel_size=kernel_size,stride=stride)\n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,bidirectional =bool(directional-1),batch_first=True)\n",
    "        self.dp = nn.Dropout(p)\n",
    "        \n",
    "        self.attention = attention(hidden_size,hidden_size,directional)\n",
    "        self.emo_linear=nn.Linear(hidden_size*directional+1024,EMOTION_NUM) \n",
    "       \n",
    "        self.out_dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=x.float()\n",
    "        \n",
    "        # conv\n",
    "        #convx=x[:,1].unsqueeze(1)\n",
    "        convx = self.conv2d(x)\n",
    "        convx = torch.flatten(convx,1) \n",
    "        #print(convx.shape)\n",
    "        \n",
    "        #GRU\n",
    "        x = self.mp(x)   # input x:(N,C,freq,time)\n",
    "        x = torch.squeeze(x)\n",
    "        x = x.permute(0,2,1)  # batch_size, time, freq\n",
    "        #x = torch.cat([x[:,0],x[:,1],x[:,2]],dim=2)\n",
    "        \n",
    "        # gru_output(batch_size, sequence length, hidden_size*directional)\n",
    "        # gru_h(batch_size,n_layer*directional,hidden_size)\n",
    "        # gru_h=[for_1,back_1,for_2,back_2...] \n",
    "        \n",
    "        gru_output,gru_h=self.gru(x) # gru_output(B, S, H*2), gru_h(n_layer*directional,B,H)\n",
    "        gru_h=gru_h.permute(1,0,2)\n",
    "        gru_output = self.dp(gru_output)\n",
    "        attention_weight = self.attention(gru_output,gru_h) # (batch_size,1,src_len)\n",
    "        \n",
    "        # (batch_size,1,src_len)*(batch_size,src_len,hidden_size*directional)\n",
    "        # (batch_size,1,hidden_size*directional)\n",
    "        attention_weight = attention_weight.unsqueeze(1)\n",
    "#         print(attention_weight.shape)\n",
    "#         print(gru_output.shape)\n",
    "        attention = torch.bmm(attention_weight,gru_output)\n",
    "        attention = torch.squeeze(attention,1) #（batch_size,hidden_size*directional）\n",
    "        \n",
    "        # cat\n",
    "        output = torch.cat([convx,attention],dim=1)\n",
    "        #print(attention.shape)\n",
    "        #print(output.shape)\n",
    "        output = self.out_dropout(self.emo_linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd35a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=1e-3,momentum=0.5)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "loss_set=[]\n",
    "accuracy_train=[]\n",
    "accuracy_validate=[]\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx,(input,target) in enumerate(train_loader,0):\n",
    "        input,target = input.to(device),target.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output=model(input)\n",
    "        _,pre = torch.max(output.data,dim=1)\n",
    "        correct += (pre==target).sum().item()\n",
    "        total += target.size(0)\n",
    "        target = target.long()\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        loss_set.append(loss.item())\n",
    "        if batch_idx % 20 == 19:\n",
    "            print('[%d,%5d] loss: %.5f, running_loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / min((batch_idx+1)*BATCH_SIZE,df.shape[0]), running_loss))\n",
    "            running_loss = 0\n",
    "    loss_set.append(running_loss)\n",
    "    accuracy_train.append(correct / total)\n",
    "    print('Accacy on train_loader set: %d %% [%d/%d]' % (100 * correct / total, correct, total))   \n",
    "      \n",
    "#total = 0\n",
    "#correct = 0\n",
    "def validate(loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for (input,target) in (eval_loader if loader=='eval_loader' else test_loader):\n",
    "        model.eval()\n",
    "        input,target  =input.to(device),target.to(device)\n",
    "        output = model(input)\n",
    "        _,pre = torch.max(output.data,dim=1)\n",
    "        correct += (pre==target).sum().item()\n",
    "        total += target.size(0)  \n",
    "    currect_rate = correct / total\n",
    "    print('Accacy on %s set: %d %% [%d/%d]' % (loader,100 * correct / total, correct, total)) \n",
    "    accuracy_validate.append(currect_rate)\n",
    "    return currect_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42c183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='model\\\\model4.pt' \n",
    "opti_path='model\\\\opti4.pt' \n",
    "loss_path='model\\\\loss4.txt'\n",
    "accur_train_path='model\\\\accur_train4.txt'\n",
    "accur_valid_path='model\\\\accur_valid4.txt'\n",
    "def saveModel(model_path):\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "    torch.save(optimizer.state_dict(),opti_path) \n",
    "#     print(\"Model's state_dict:\")\n",
    "#     for param_tensor in model.state_dict():\n",
    "#         print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "        \n",
    "def loadModel(model_path):\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "def saveData(loss,accuracy_train,accuracy_validate):\n",
    "    np.savetxt(loss_path,np.array(loss),fmt='%0.8f')\n",
    "    np.savetxt(accur_train_path,np.array(accuracy_train),fmt='%0.8f')\n",
    "    np.savetxt(accur_valid_path,np.array(accuracy_validate),fmt='%0.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad8f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   20] loss: 0.07471, running_loss: 47.812\n",
      "Accacy on train_loader set: 21 % [252/1152]\n",
      "Accacy on eval_loader set: 16 % [24/144]\n",
      "[2,   20] loss: 0.05883, running_loss: 37.650\n",
      "Accacy on train_loader set: 31 % [365/1152]\n",
      "Accacy on eval_loader set: 15 % [22/144]\n",
      "[3,   20] loss: 0.05535, running_loss: 35.426\n",
      "Accacy on train_loader set: 35 % [414/1152]\n",
      "Accacy on eval_loader set: 15 % [23/144]\n",
      "[4,   20] loss: 0.05072, running_loss: 32.458\n",
      "Accacy on train_loader set: 39 % [450/1152]\n",
      "Accacy on eval_loader set: 15 % [22/144]\n",
      "[5,   20] loss: 0.04798, running_loss: 30.707\n",
      "Accacy on train_loader set: 40 % [468/1152]\n",
      "Accacy on eval_loader set: 15 % [23/144]\n",
      "[6,   20] loss: 0.04931, running_loss: 31.556\n",
      "Accacy on train_loader set: 42 % [492/1152]\n",
      "Accacy on eval_loader set: 18 % [27/144]\n",
      "[7,   20] loss: 0.04668, running_loss: 29.877\n",
      "Accacy on train_loader set: 44 % [513/1152]\n",
      "Accacy on eval_loader set: 18 % [26/144]\n",
      "[8,   20] loss: 0.04550, running_loss: 29.122\n",
      "Accacy on train_loader set: 45 % [522/1152]\n",
      "Accacy on eval_loader set: 20 % [29/144]\n",
      "[9,   20] loss: 0.04271, running_loss: 27.334\n",
      "Accacy on train_loader set: 46 % [541/1152]\n",
      "Accacy on eval_loader set: 21 % [31/144]\n",
      "[10,   20] loss: 0.04213, running_loss: 26.964\n",
      "Accacy on train_loader set: 48 % [562/1152]\n",
      "Accacy on eval_loader set: 21 % [31/144]\n",
      "[11,   20] loss: 0.04247, running_loss: 27.181\n",
      "Accacy on train_loader set: 49 % [568/1152]\n",
      "Accacy on eval_loader set: 22 % [33/144]\n",
      "[12,   20] loss: 0.04152, running_loss: 26.570\n",
      "Accacy on train_loader set: 48 % [553/1152]\n",
      "Accacy on eval_loader set: 22 % [32/144]\n",
      "[13,   20] loss: 0.04258, running_loss: 27.252\n",
      "Accacy on train_loader set: 48 % [557/1152]\n",
      "Accacy on eval_loader set: 22 % [32/144]\n",
      "[14,   20] loss: 0.04165, running_loss: 26.654\n",
      "Accacy on train_loader set: 50 % [580/1152]\n",
      "Accacy on eval_loader set: 26 % [38/144]\n",
      "[15,   20] loss: 0.03941, running_loss: 25.225\n",
      "Accacy on train_loader set: 52 % [609/1152]\n",
      "Accacy on eval_loader set: 20 % [30/144]\n",
      "[16,   20] loss: 0.04068, running_loss: 26.037\n",
      "Accacy on train_loader set: 50 % [578/1152]\n",
      "Accacy on eval_loader set: 25 % [37/144]\n",
      "[17,   20] loss: 0.03811, running_loss: 24.392\n",
      "Accacy on train_loader set: 54 % [627/1152]\n",
      "Accacy on eval_loader set: 29 % [42/144]\n",
      "[18,   20] loss: 0.03687, running_loss: 23.596\n",
      "Accacy on train_loader set: 53 % [618/1152]\n",
      "Accacy on eval_loader set: 29 % [42/144]\n",
      "[19,   20] loss: 0.03786, running_loss: 24.231\n",
      "Accacy on train_loader set: 54 % [629/1152]\n",
      "Accacy on eval_loader set: 29 % [42/144]\n",
      "[20,   20] loss: 0.03810, running_loss: 24.383\n",
      "Accacy on train_loader set: 54 % [629/1152]\n",
      "Accacy on eval_loader set: 35 % [51/144]\n",
      "[21,   20] loss: 0.03669, running_loss: 23.484\n",
      "Accacy on train_loader set: 55 % [643/1152]\n",
      "Accacy on eval_loader set: 31 % [46/144]\n",
      "[22,   20] loss: 0.03583, running_loss: 22.931\n",
      "Accacy on train_loader set: 56 % [653/1152]\n",
      "Accacy on eval_loader set: 32 % [47/144]\n",
      "[23,   20] loss: 0.03610, running_loss: 23.104\n",
      "Accacy on train_loader set: 56 % [648/1152]\n",
      "Accacy on eval_loader set: 34 % [50/144]\n",
      "[24,   20] loss: 0.03545, running_loss: 22.687\n",
      "Accacy on train_loader set: 58 % [674/1152]\n",
      "Accacy on eval_loader set: 31 % [45/144]\n",
      "[25,   20] loss: 0.03390, running_loss: 21.695\n",
      "Accacy on train_loader set: 57 % [666/1152]\n",
      "Accacy on eval_loader set: 41 % [60/144]\n",
      "[26,   20] loss: 0.03532, running_loss: 22.605\n",
      "Accacy on train_loader set: 58 % [670/1152]\n",
      "Accacy on eval_loader set: 37 % [54/144]\n",
      "[27,   20] loss: 0.03399, running_loss: 21.754\n",
      "Accacy on train_loader set: 57 % [667/1152]\n",
      "Accacy on eval_loader set: 32 % [47/144]\n",
      "[28,   20] loss: 0.03420, running_loss: 21.885\n",
      "Accacy on train_loader set: 59 % [680/1152]\n",
      "Accacy on eval_loader set: 39 % [57/144]\n",
      "[29,   20] loss: 0.03288, running_loss: 21.040\n",
      "Accacy on train_loader set: 60 % [700/1152]\n",
      "Accacy on eval_loader set: 34 % [50/144]\n",
      "[30,   20] loss: 0.03371, running_loss: 21.575\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    max_correct_rate=0.0\n",
    "    for epoch in range(1500):\n",
    "        train(epoch)\n",
    "        correct_rate=validate('eval_loader')\n",
    "        if max_correct_rate < correct_rate:\n",
    "            max_correct_rate = correct_rate\n",
    "            saveModel(model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_correct_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    for epoch in range(3500):\n",
    "        train(epoch)\n",
    "        correct_rate=validate('eval_loader')\n",
    "        if max_correct_rate < correct_rate:\n",
    "            max_correct_rate = correct_rate\n",
    "            saveModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_correct_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model=loadModel(model_path)\n",
    "    validate('test_loader')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
