{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd82d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "EMOTIONS = {1:\"neu\",2:'cal',3:'hap',4:'sad',5:'ang',6:'fea',7:'dis',8:'sur'}\n",
    "EMOTION_NUM = 8\n",
    "TILTLEEMOTIONS={\"neu\":'中性','cal':'平静','hap':'快乐','sad':'悲伤','ang':'生气','fea':'害怕','dis':'厌恶','sur':'惊讶'}\n",
    "SOURCE_PATH = \"db/RAVDESS/\"\n",
    "SAMPLE_RATE = 48000\n",
    "EPOCH=10\n",
    "FILE_PATH=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc04ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['FILE','EMOTION',])\n",
    "FILE_NAME=[]\n",
    "EMOTIONLIST=[]\n",
    "for director, _, file_names in os.walk(SOURCE_PATH):\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(director+'/',file_name)\n",
    "        FILE_NAME.append(file_path)\n",
    "        EMOTIONLIST.append(file_name.split('.')[0].split('-')[2])\n",
    "df[\"FILE\"]=FILE_NAME\n",
    "df[\"EMOTION\"]=EMOTIONLIST\n",
    "\n",
    "print(\"共有{}行\".format(df.shape[0]))\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cbdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_eval_index,test_index=train_test_split(list(df.index),test_size=0.3,random_state=1)\n",
    "train_index,eval_index=train_test_split(train_eval_index,test_size=0.125,random_state=1)\n",
    "\n",
    "print(\"train_index前10个:\\t\",train_index[:10])\n",
    "print(\"eval_index前10个:\\t\",eval_index[:10])\n",
    "print(\"test_index前十个:\\t\",test_index[:10])\n",
    "print('test_index/(train_index+test_index)={}/({}+{})={}'.format(len(test_index),len(train_eval_index),len(test_index),len(test_index)/(len(test_index)+len(train_eval_index))))\n",
    "print('eval_index/(train_index+eval_index)={}/({}+{})={}'.format(len(eval_index),len(train_index),len(eval_index),len(eval_index)/(len(eval_index)+len(train_index))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86add56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def getMELspec(data,sr):\n",
    "    # shape=(n_mels, t)\n",
    "    mel_spec = librosa.feature.melspectrogram(y = data,\n",
    "                                              sr = SAMPLE_RATE,\n",
    "                                              n_fft=1024,      # length of the FFT window\n",
    "                                              win_length=512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length=256,\n",
    "                                              n_mels=128\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec,ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def labelEncoder():   \n",
    "    encoder = OneHotEncoder()\n",
    "    labels = encoder.fit_transform(np.array(df['EMOTION']).reshape(-1, 1)).toarray()\n",
    "    # np.set_printoptions(threshold=np.inf)\n",
    "    return labels\n",
    "\n",
    "y_data = labelEncoder() \n",
    "print(np.array(y_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.97\n",
    "def PreEmphsised(data):\n",
    "    return np.append(data[0],data[1:] - alpha * data[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "DURATION=3\n",
    "OFFSET=0.5\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self,index):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        with tqdm(total=len(index),desc=\"数据加载\") as loadbar:\n",
    "            for i in index: # 写入1440个文件的数据\n",
    "                data, sr = librosa.load(df['FILE'][i],sr=SAMPLE_RATE,duration=DURATION,offset=OFFSET)\n",
    "                smLenData = np.zeros(SAMPLE_RATE*DURATION)\n",
    "                smLenData[0:len(data)]=data\n",
    "                # 预加重\n",
    "                smLenData = PreEmphsised(smLenData)\n",
    "                # 特征向量\n",
    "                mel_data = getMELspec(smLenData,sr)\n",
    "                self.x_data.append(list(mel_data))\n",
    "                self.y_data.append(y_data[i])\n",
    "                loadbar.update(1)\n",
    "        shape=np.array(self.x_data).shape\n",
    "        self.x_data = np.reshape(self.x_data,newshape=(shape[0],-1))\n",
    "        self.x_data = self.scaler.fit_transform(self.x_data)\n",
    "        self.x_data=np.reshape(self.x_data,newshape=shape)\n",
    "        \n",
    "        self.x_data=torch.from_numpy(np.array(self.x_data,dtype=np.float64))\n",
    "        self.y_data=torch.from_numpy(np.array(self.y_data,dtype=np.float64))\n",
    "  \n",
    "        self.len = len(index)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50455ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "# 训练集加载\n",
    "train_dataset = SpeechDataset(train_index)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2\n",
    "                         )\n",
    "# 验证集加载\n",
    "eval_dataset = SpeechDataset(eval_index)\n",
    "eval_loader = DataLoader(eval_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2\n",
    "                         )\n",
    "# 测试集加载\n",
    "test_dataset = SpeechDataset(test_index)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dc604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class attention(nn.Module):\n",
    "    def __init__(self,en_hidden_dim,de_hidden_dim,direction):\n",
    "        super().__init__()\n",
    "        middle_size = de_hidden_dim\n",
    "        # H与s0拼接出来为(batch_size,src_len,en_hidden_dim*direction+de_hidden_dim)\n",
    "        # 需要转化维度。首先，a(batch_size,src_len),公式2得知v(batch_size,?),E(?,src_len)\n",
    "        # 因此?可以是任意维度，此处取de_hidden_dim\n",
    "        self.s_en2de=nn.Linear(en_hidden_dim*2,de_hidden_dim)\n",
    "        \n",
    "        self.attn=nn.Linear(en_hidden_dim*direction+de_hidden_dim,middle_size)\n",
    "        \n",
    "        self.v = nn.Linear(middle_size,1)\n",
    "        \n",
    "    def forward(self,gru_output,gru_hidden):       \n",
    "        # H(batch_size,src_len,en_hidden_dim*direction)\n",
    "        _,src_len,_=gru_output.shape\n",
    "        # s0(batch_size,en_hidden_dim*direction)\n",
    "        # 但是s0作为dcoder初始隐藏状态，应该是s0(batch_size,de_hidden_dim)\n",
    "        # 并且为了与H拼接，需要加一个维度，变成s0(batch_size,src_len,de_hidden_dim)\n",
    "        s = torch.cat((gru_hidden[:,-2:,],gru_hidden[:,-1:]),dim=1)  # (batch_size,en_hidden_dim*direction)\n",
    "        s = self.s_en2de(s)  # (batch_size,de_hidden_dim)\n",
    "        # s=torc.tanh(s)\n",
    "        \n",
    "        s=s.unqueeze(1).repeat(1,src_len,1) # (batch_size,src_len,de_hidden_dim)\n",
    "        \n",
    "        self.attn_hidden = torch.tanh(self.attn(torch.cat((s,gru_output),dim=2)))\n",
    "        \n",
    "        attenion_weight = self.v(self.attn_hidden) #(batch_size,1,src_len)\n",
    "        \n",
    "        return nn.softmax(attention_weight,dim=2)   #A dimension along which Softmax will be computed (so every slice along dim will sum to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
